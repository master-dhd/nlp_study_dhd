# -*- coding: utf-8 -*-

import torch
import time
import json
import logging
from transformers import AutoModelForCausalLM, AutoTokenizer
from torch.utils.data import DataLoader
from loader_qwen import load_qwen_data
import numpy as np
from collections import defaultdict

"""
Qwenæ¨¡å‹è¯„ä¼°å™¨
ç”¨äºè¯„ä¼°æ ‡é¢˜é¢„æµ‹æ–‡ç« å†…å®¹çš„ç”Ÿæˆæ•ˆæœ
"""

class QwenEvaluator:
    def __init__(self, config, model, logger):
        self.config = config
        self.model = model
        self.logger = logger
        
        # åˆå§‹åŒ–tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(
            config["bert_model_path"], 
            trust_remote_code=True
        )
        
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # åŠ è½½éªŒè¯æ•°æ®
        self.eval_dataloader = load_qwen_data(
            config["eval_data_path"], 
            config, 
            logger, 
            shuffle=False
        )
        
        self.device = next(model.parameters()).device
    
    def eval(self, epoch):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        """
        print(f"\nğŸ“Š å¼€å§‹ç¬¬ {epoch} è½®è¯„ä¼°...")
        eval_start_time = time.time()
        
        self.model.eval()
        total_loss = 0.0
        total_samples = 0
        generation_examples = []
        
        with torch.no_grad():
            for batch_idx, batch in enumerate(self.eval_dataloader):
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch["input_ids"].to(self.device)
                attention_mask = batch["attention_mask"].to(self.device)
                labels = batch["labels"].to(self.device)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )
                
                loss = outputs.loss
                total_loss += loss.item() * input_ids.size(0)
                total_samples += input_ids.size(0)
                
                # æ”¶é›†ç”Ÿæˆç¤ºä¾‹ï¼ˆå‰3ä¸ªæ‰¹æ¬¡ï¼‰
                if batch_idx < 3:
                    self._collect_generation_examples(
                        input_ids, attention_mask, batch_idx, generation_examples
                    )
                
                # æ˜¾ç¤ºè¿›åº¦
                if (batch_idx + 1) % 10 == 0:
                    current_loss = total_loss / total_samples
                    print(f"   æ‰¹æ¬¡ {batch_idx + 1}/{len(self.eval_dataloader)}, å½“å‰å¹³å‡æŸå¤±: {current_loss:.4f}")
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_loss = total_loss / total_samples
        eval_time = time.time() - eval_start_time
        
        # æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        self._display_evaluation_results(epoch, avg_loss, eval_time, generation_examples)
        
        self.model.train()
        return avg_loss
    
    def _collect_generation_examples(self, input_ids, attention_mask, batch_idx, examples):
        """
        æ”¶é›†ç”Ÿæˆç¤ºä¾‹
        """
        try:
            # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
            sample_input = input_ids[0:1]
            sample_mask = attention_mask[0:1]
            
            # æ‰¾åˆ°"å†…å®¹:"çš„ä½ç½®ï¼Œåªä¿ç•™æ ‡é¢˜éƒ¨åˆ†ä½œä¸ºè¾“å…¥
            input_text = self.tokenizer.decode(sample_input[0], skip_special_tokens=True)
            if "å†…å®¹:" in input_text:
                title_part = input_text.split("å†…å®¹:")[0] + "å†…å®¹:"
                
                # é‡æ–°ç¼–ç æ ‡é¢˜éƒ¨åˆ†
                title_inputs = self.tokenizer(
                    title_part, 
                    return_tensors="pt", 
                    padding=True, 
                    truncation=True, 
                    max_length=50
                ).to(self.device)
                
                # ç”Ÿæˆå†…å®¹
                with torch.no_grad():
                    generated = self.model.generate(
                        **title_inputs,
                        max_new_tokens=80,
                        do_sample=True,
                        temperature=0.7,
                        top_p=0.9,
                        pad_token_id=self.tokenizer.eos_token_id
                    )
                
                # è§£ç ç”Ÿæˆçš„æ–‡æœ¬
                generated_text = self.tokenizer.decode(generated[0], skip_special_tokens=True)
                original_text = self.tokenizer.decode(sample_input[0], skip_special_tokens=True)
                
                examples.append({
                    'batch_idx': batch_idx,
                    'original': original_text,
                    'generated': generated_text,
                    'title_part': title_part
                })
                
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆç¤ºä¾‹æ—¶å‡ºé”™: {e}")
    
    def _display_evaluation_results(self, epoch, avg_loss, eval_time, examples):
        """
        æ˜¾ç¤ºè¯„ä¼°ç»“æœ
        """
        print(f"\nğŸ“ˆ ç¬¬ {epoch} è½®è¯„ä¼°ç»“æœ:")
        print(f"   å¹³å‡æŸå¤±: {avg_loss:.4f}")
        print(f"   è¯„ä¼°è€—æ—¶: {eval_time:.2f} ç§’")
        print(f"   æ ·æœ¬æ•°é‡: {len(self.eval_dataloader.dataset)}")
        
        # æ˜¾ç¤ºç”Ÿæˆç¤ºä¾‹
        if examples:
            print(f"\nğŸ“ ç”Ÿæˆç¤ºä¾‹:")
            for i, example in enumerate(examples[:3], 1):
                print(f"\n   ç¤ºä¾‹ {i}:")
                
                # æå–æ ‡é¢˜
                if "æ ‡é¢˜:" in example['original']:
                    title = example['original'].split("æ ‡é¢˜:")[1].split("\n")[0].strip()
                    print(f"     æ ‡é¢˜: {title}")
                
                # æå–åŸå§‹å†…å®¹
                if "å†…å®¹:" in example['original']:
                    original_content = example['original'].split("å†…å®¹:")[1].strip()
                    print(f"     åŸå§‹å†…å®¹: {original_content[:100]}{'...' if len(original_content) > 100 else ''}")
                
                # æå–ç”Ÿæˆå†…å®¹
                if "å†…å®¹:" in example['generated']:
                    generated_content = example['generated'].split("å†…å®¹:")[1].strip()
                    print(f"     ç”Ÿæˆå†…å®¹: {generated_content[:100]}{'...' if len(generated_content) > 100 else ''}")
        
        # æ˜¾ç¤ºGPUå†…å­˜ä½¿ç”¨æƒ…å†µ
        if torch.cuda.is_available():
            allocated = torch.cuda.memory_allocated() / 1024**3
            cached = torch.cuda.memory_reserved() / 1024**3
            print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨: {allocated:.2f} GB (ç¼“å­˜: {cached:.2f} GB)")
        
        print("â”€" * 50)
    
    def generate_sample(self, title, max_length=100):
        """
        æ ¹æ®æ ‡é¢˜ç”Ÿæˆå†…å®¹ç¤ºä¾‹
        """
        prompt = f"æ ‡é¢˜: {title}\nå†…å®¹: "
        
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt", 
            padding=True, 
            truncation=True, 
            max_length=50
        ).to(self.device)
        
        self.model.eval()
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_length,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=self.tokenizer.eos_token_id
            )
        
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–ç”Ÿæˆçš„å†…å®¹éƒ¨åˆ†
        if "å†…å®¹:" in generated_text:
            content = generated_text.split("å†…å®¹:")[1].strip()
            return content
        
        return generated_text
    
    def batch_evaluate_titles(self, titles):
        """
        æ‰¹é‡è¯„ä¼°æ ‡é¢˜ç”Ÿæˆæ•ˆæœ
        """
        print(f"\nğŸ” æ‰¹é‡ç”Ÿæˆæµ‹è¯• ({len(titles)} ä¸ªæ ‡é¢˜)...")
        
        results = []
        total_time = 0
        
        for i, title in enumerate(titles, 1):
            print(f"\nğŸ“ {i}/{len(titles)}: {title}")
            
            start_time = time.time()
            try:
                content = self.generate_sample(title)
                gen_time = time.time() - start_time
                total_time += gen_time
                
                print(f"   â±ï¸  è€—æ—¶: {gen_time:.2f}s")
                print(f"   ğŸ“„ å†…å®¹: {content[:150]}{'...' if len(content) > 150 else ''}")
                
                results.append({
                    'title': title,
                    'content': content,
                    'time': gen_time,
                    'success': True
                })
                
            except Exception as e:
                gen_time = time.time() - start_time
                total_time += gen_time
                print(f"   âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
                
                results.append({
                    'title': title,
                    'content': '',
                    'time': gen_time,
                    'success': False
                })
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if r['success'])
        avg_time = total_time / len(results)
        
        print(f"\nğŸ“Š æ‰¹é‡æµ‹è¯•ç»“æœ:")
        print(f"   æˆåŠŸç‡: {successful}/{len(titles)} ({successful/len(titles)*100:.1f}%)")
        print(f"   å¹³å‡è€—æ—¶: {avg_time:.2f} ç§’")
        print(f"   æ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        return results


if __name__ == "__main__":
    from config_qwen import Config
    import logging
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ ç‹¬ç«‹çš„è¯„ä¼°æµ‹è¯•ä»£ç 
    print("QwenEvaluator æ¨¡å—å·²åŠ è½½")
